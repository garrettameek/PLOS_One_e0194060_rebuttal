<!DOCTYPE html>
<html lang="en" dir="ltr" class="client-nojs">
<head>
<meta charset="UTF-8" />
<title>454 SOP - mothur</title>
<meta name="generator" content="MediaWiki 1.25.1" />
<link rel="shortcut icon" href="/favicon.ico" />
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="mothur (en)" />
<link rel="EditURI" type="application/rsd+xml" href="https://mothur.org/w/api.php?action=rsd" />
<link rel="alternate" hreflang="x-default" href="/wiki/454_SOP" />
<link rel="alternate" type="application/atom+xml" title="mothur Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
<link rel="stylesheet" href="https://mothur.org/w/load.php?debug=false&amp;lang=en&amp;modules=mediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.sectionAnchor%7Cmediawiki.skinning.interface%7Cmediawiki.ui.button%7Cskins.vector.styles&amp;only=styles&amp;skin=vector&amp;*" />
<meta name="ResourceLoaderDynamicStyles" content="" />
<link rel="stylesheet" href="https://mothur.org/w/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=styles&amp;skin=vector&amp;*" />
<style>a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}
/* cache key: wikidb:resourceloader:filter:minify-css:7:b36fd1c042133c9c9b60260f7c29b237 */</style>
<script src="https://mothur.org/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector&amp;*"></script>
<script>if(window.mw){
mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"454_SOP","wgTitle":"454 SOP","wgCurRevisionId":100660,"wgRevisionId":100660,"wgArticleId":1516,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"454_SOP","wgRelevantArticleId":1516,"wgIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[]});
}</script><script>if(window.mw){
mw.loader.implement("user.options",function($,jQuery){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens",function($,jQuery){mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\"});});
/* cache key: wikidb:resourceloader:filter:minify-js:7:a5c52c063dc436c1ca7c9f456936a5e9 */
}</script>
<script>if(window.mw){
mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","mediawiki.legacy.ajax","skins.vector.js"]);
}</script>
<!--[if lt IE 7]><style type="text/css">body{behavior:url("/w/skins/Vector/csshover.min.htc")}</style><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-454_SOP skin-vector action-view">
		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>

							<div id="siteNotice"><div id="localNotice" lang="en" dir="ltr"><h3><span class="mw-headline" id="We_will_be_offering_mothur_and_R_workshops_throughout_2019._Learn_more.">We will be offering mothur and R workshops throughout 2019.  <a href="/wiki/Workshops" title="Workshops"> Learn more.</a></span></h3>
</div></div>
						<div class="mw-indicators">
</div>
			<h1 id="firstHeading" class="firstHeading" lang="en">454 SOP</h1>
						<div id="bodyContent" class="mw-body-content">
									<div id="siteSub">From mothur</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-head">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><p><b>NOTE:</b> Although this is an SOP, it is something of a work in progress and continues to be modified as we learn more. If you are using this protocol in a paper, you must cite the Schloss et al. 2011 PLoS ONE paper and cite the date you accessed this page:
</p><p><br />
Schloss PD, Gevers D, Westcott SL. (2011). Reducing the effects of PCR amplification and sequencing artifacts on 16S rRNA-based studies. PloS ONE. 6:e27310.
</p><p><br />
This goal of this tutorial is to demonstrate the standard operating procedure (SOP) that the Schloss lab uses to process their 16S rRNA gene sequences that are generated by 454 pyrosequencing. Throughout the tutorial several alternative approaches will be mentioned along with why we might deviate from the SOP to use these approaches. This SOP is largely the product of a series of manuscripts that we have published and users are advised to consult these for more details and background data. The workflow is being divided into several parts shown here in the table of contents for the tutorial:
</p><p><br />
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Logistics"><span class="tocnumber">1</span> <span class="toctext">Logistics</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Getting_started"><span class="tocnumber">2</span> <span class="toctext">Getting started</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Reducing_sequencing_error"><span class="tocnumber">3</span> <span class="toctext">Reducing sequencing error</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Using_flowgrams"><span class="tocnumber">3.1</span> <span class="toctext">Using flowgrams</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Using_quality_scores"><span class="tocnumber">3.2</span> <span class="toctext">Using quality scores</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#Processing_improved_sequences"><span class="tocnumber">4</span> <span class="toctext">Processing improved sequences</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Removing_chimeras"><span class="tocnumber">5</span> <span class="toctext">Removing chimeras</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Removing_.22contaminants.22"><span class="tocnumber">6</span> <span class="toctext">Removing "contaminants"</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Error_analysis"><span class="tocnumber">7</span> <span class="toctext">Error analysis</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#Preparing_inputs_for_analysis"><span class="tocnumber">8</span> <span class="toctext">Preparing inputs for analysis</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="#OTUs"><span class="tocnumber">8.1</span> <span class="toctext">OTUs</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Phylotype"><span class="tocnumber">8.2</span> <span class="toctext">Phylotype</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Phylogenetic_tree"><span class="tocnumber">8.3</span> <span class="toctext">Phylogenetic tree</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="#Analysis"><span class="tocnumber">9</span> <span class="toctext">Analysis</span></a>
<ul>
<li class="toclevel-2 tocsection-15"><a href="#OTU-based_analysis"><span class="tocnumber">9.1</span> <span class="toctext">OTU-based analysis</span></a>
<ul>
<li class="toclevel-3 tocsection-16"><a href="#Alpha_diversity"><span class="tocnumber">9.1.1</span> <span class="toctext">Alpha diversity</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="#Beta_diversity_measurements"><span class="tocnumber">9.1.2</span> <span class="toctext">Beta diversity measurements</span></a></li>
<li class="toclevel-3 tocsection-18"><a href="#Population-level_analysis"><span class="tocnumber">9.1.3</span> <span class="toctext">Population-level analysis</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-19"><a href="#Phylotype-based_analysis"><span class="tocnumber">9.2</span> <span class="toctext">Phylotype-based analysis</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="#Phylogeny-based_analysis"><span class="tocnumber">9.3</span> <span class="toctext">Phylogeny-based analysis</span></a>
<ul>
<li class="toclevel-3 tocsection-21"><a href="#Alpha_diversity_2"><span class="tocnumber">9.3.1</span> <span class="toctext">Alpha diversity</span></a></li>
<li class="toclevel-3 tocsection-22"><a href="#Beta_diversity"><span class="tocnumber">9.3.2</span> <span class="toctext">Beta diversity</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-23"><a href="#Conclusion"><span class="tocnumber">10</span> <span class="toctext">Conclusion</span></a></li>
</ul>
</div>

<p><br />
</p>
<h2><span class="mw-headline" id="Logistics">Logistics</span></h2>
<p>The first step of the tutorial is to understand how we typically set up a plate of barcodes for sequencing. First, you can obtain a set of barcodes from multiple sources. We utilize the "Broad" barcodes that were used for the HMP 16S rRNA gene sequencing effort. You can find these barcodes and primers to sequence the V13, V35, and V69 regions online. Next, it is worth noting that the sequencing is done in the "reverse direction" - e.g. starting at the 3' end of the V5 region, we sequence back towards the 5' end of the V3 region. We picked the V35 region because we liked these primers the best for the types of samples that we generally process (i.e. human and mouse). You may choose to use another region based on the expected biodiversity of your samples. Second, this set of barcodes will allow us to simultaneously sequence 96 samples. A priori, we dedicate two of these barcodes to controls. The first is for resequencing a mock community made up of a defined consortia of 16S rRNA gene sequences where we know the true sequence. This is helpful in assessing sequencing errors and drift in the process. The second is a "generous donor" sample, which is a representative sample (e.g. stool) that we resequence on each plate. This provides a more realistic understanding of how processing drift might be affecting our analysis. Finally, we generally utilize half of a 454 plate for each set of 96 barcodes. A typical plate has two halves (duh) and it is not advisable to put the same barcode-primer combination on these two halves if they are used for different samples as there is leakage across the gasket.
</p><p><br />
Starting out we need to first determine, what was our question? The Schloss lab is interested in understanding the effect of normal variation in the gut microbiome on host health. To that end we collected fresh feces from mice on a daily basis for 365 days post weaning (we're accepting applications). During the first 150 days post weaning (dpw), nothing was done to our mice except allow them to eat, get fat, and be merry. We were curious whether the rapid change in weight observed during the first 10 dpw affected the stability microbiome compared to the microbiome observed between days 140 and 150. We will address this question in this tutorial using a combination of OTU, phylotype, and phylogenetic methods. To make this tutorial easier to execute, we are providing only part of the data - you are given the flow files for one animal at 10 time points (5 early and 5 late). In addition, to sequencing samples from mice fecal material, we resequenced a mock community composed of genomic DNA from 21 bacterial and archaeal strains. We will use the 10 fecal samples to look at how to analyze microbial communities and the mock community to measure the error rate and its effect on other analyses.
</p><p><br />
When we get our sequences back they arrive as an sff file. Often times people will only get back a fasta file or they may get back the fasta and qual files. You really need the sff file or at least the fasta, qual, and flow file data. If your sequence provider can't or won't provide these data find a new sequence provider. It is also critical to know the barcode and primer sequence used for each sample. We have heard reports from many users that their sequence provider has already trimmed and quality trimmed the data for them. Ahemm... don't trust them. If your sequence provider won't give you your primer or barcode sequences, move on and use someone else. For this tutorial you will need several sets of files. To speed up the tutorial we provide some of the downstream files that take awhile to generate (e.g. the output of shhh.flows):
</p><p><br />
</p>
<ul><li> <a href="/w/images/a/a1/SOPData.zip" class="internal" title="SOPData.zip"> Example data from Schloss lab</a> that will be used with this tutorial.  It was extracted from the very large <a href="/w/images/2/27/SOP_SFF.bz2" class="internal" title="SOP SFF.bz2"> full SFF file</a></li>
<li> <a href="/wiki/Lookup_files" title="Lookup files"> Lookup data for use with shhh.flows</a></li>
<li> <a href="/w/images/9/98/Silva.bacteria.zip" class="internal" title="Silva.bacteria.zip"> SILVA-based bacterial reference alignment</a></li>
<li> <a href="/w/images/5/59/Trainset9_032012.pds.zip" class="internal" title="Trainset9 032012.pds.zip"> mothur-formatted version of the RDP training set (v.9)</a></li></ul>
<p><br />
It is generally easiest to decompress these files and to then move the contents of the Trainset9_032012.pds and the silva.bacteria folders into the Schloss_SOP folder.  You will also want to move the contents of the mothur executable folder there as well.  If you are a sysadmin wiz (or novice) you can probably figure out how to put mothur in your path, but this will get you what you need for now.
</p><p><br />
In addition, you probably want to get your hands on the following...
</p>
<ul><li> <a rel="nofollow" class="external text" href="http://www.mono-project.com/Main_Page">mono</a> - if you are using Mac OS X or linux</li>
<li> <a rel="nofollow" class="external text" href="http://www.northeastern.edu/catchall/downloads.html">CatchAll</a></li>
<li> <a rel="nofollow" class="external text" href="http://www.barebones.com/products/textwrangler/">TextWranger</a> / <a rel="nofollow" class="external text" href="http://www.gnu.org/software/emacs/">emacs</a> / <a rel="nofollow" class="external text" href="http://www.vim.org/">vi</a> / or some other text editor</li>
<li> <a rel="nofollow" class="external text" href="http://www.r-project.org/">R</a>, Excel, or another program to graph data</li>
<li> Adobe Illustrator, Safari, or <a rel="nofollow" class="external text" href="http://inkscape.org/">Inkscape</a></li>
<li> <a rel="nofollow" class="external text" href="http://taxonomy.zoology.gla.ac.uk/rod/treeview.html">TreeView</a>, <a rel="nofollow" class="external text" href="http://tree.bio.ed.ac.uk/software/figtree/">FigTree</a>, <a rel="nofollow" class="external text" href="http://topiaryexplorer.sourceforge.net/">Topiary Explorer</a> or another program to visualize dendrograms</li></ul>
<p>It is generally easiest to use the "current" option for many of the commands since the file names get very long.  Because this tutorial is meant to show people how to use mothur at a very nuts and bolts level, we will only selectively use the current option to demonstrate how it works.  Generally, we will use the full file names.
</p>
<h2><span class="mw-headline" id="Getting_started">Getting started</span></h2>
<p>Because of the large size of the original sff file (~800 MB) and that the data are not published, the folder you pulled down above does not contain the sff file. Instead we are giving you 21 of the 96 flow files that were outputted from the first several commands here. This tutorial will tell you when to pick up with the data in the folder...
</p><p><br />
As mentioned above we get data in the form of an sff file and will use mothur's implementation of <a href="/wiki/Sffinfo" title="Sffinfo">sffinfo</a> to extract the fasta, qual, and flow data from the binary file:
</p>
<pre>mothur &gt; sffinfo(sff=GQY1XT001.sff, flow=T)
</pre>
<p>Sometimes it is not possible to get the sff file because your sequence provider is sketchy or because you are pulling data down from some other scientist's study who didn't provide the data. In those cases you may need to skip that step. Regardless, let's see what our sequences look like that are in the fasta file usig the <a href="/wiki/Summary.seqs" title="Summary.seqs">summary.seqs</a> command:
</p>
<pre>mothur &gt; summary.seqs(fasta=GQY1XT001.fasta)

		Start	End	NBases	Ambigs	Polymer
Minimum:	1	39	39	0	2
2.5%-tile:	1	116	116	0	4
25%-tile:	1	454	454	0	4
Median: 	1	512	512	0	5
75%-tile:	1	539	539	0	5
97.5%-tile:	1	558	558	1	6
Maximum:	1	1040	1040	28	31
# of Seqs:	763373
</pre>
<p>This is the sequence profile for the entire half plate. In the flow files we have provided to you, there are a total of ~69000 sequences.
</p><p><br />
</p>
<h2><span class="mw-headline" id="Reducing_sequencing_error">Reducing sequencing error</span></h2>
<h3><span class="mw-headline" id="Using_flowgrams">Using flowgrams</span></h3>
<p>Our SOP will use the <a href="/wiki/Shhh.flows" title="Shhh.flows">shhh.flows</a> command, which is the mothur implementation of the PyroNoise component of the AmpliconNoise suite of programs. Others that don't have the computational capacity or don't have their flow data may want to jump ahead to the trim.seqs section of the tutorial. The rest of the SOP will assume that you are using the SOP approach.
</p><p><br />
To run shhh.flows, we need to process our flow data to do several things. First, we need to separate each flowgram according to the barcode and primer combination. We also need to make sure that our sequences are a minimum length and we need to cap the the number of flowgrams to that length. The default of <a href="/wiki/Trim.flows" title="Trim.flows">trim.flows</a> is 450 flows. If you are using GSFLX data you will probably want to play around with something in the order of 300-350 flows. We will also allow for 1 mismatch to the barcode and 2 mismatches to the primer. Running <a href="/wiki/Trim.flows" title="Trim.flows">trim.flows</a> will make use of the <a href="/wiki/Oligos_File" title="Oligos File">Oligos_File</a> that is provided with the SOP Data that you pulled down from the wiki. You may want to change the number of processors to suit your hardware:
</p>
<pre>mothur &gt; trim.flows(flow=GQY1XT001.flow, oligos=GQY1XT001.oligos, pdiffs=2, bdiffs=1, processors=2)
</pre>
<p>This will create 96 separate trim.flows and scrap.flows files as well as a file called GQY1XT001.flow.files, which holds the names of each of the trim.flows files. The folder you have pulled down has 21 of these flow files along with a file called GQY1XT001.flow.files. Next we will want to run <a href="/wiki/Shhh.flows" title="Shhh.flows">shhh.flows</a> to de-noise our sequence data. If you don't want take the time to run shhh.flows, you can use the processed files that are in the folder you pulled down above. shhh.flows can be run several ways depending on your hardware or operating system. The first way is to use the precomputed output for this dataset that came with the files you downloaded. Clearly this won't work for your own data&#160;:). Alternatively, we can stay within mothur and run the following:
</p>
<pre>mothur &gt; shhh.flows(file=GQY1XT001.flow.files, processors=2)
</pre>
<p>It took me about 8 hours (wall time) to run shhh.flows on the 96 samples in GQY1XT001.flow.files with 8 processors. Note that if you are using data recently generated on the new 454 platform and chemistry there is a different flow order. If this is your case, you'll want to include order=B in trim.flows and shhh.flows. If you're using IonTorrent (why!?), you'll want to use order=I.
</p><p><br />
The two important files from running shhh.flows are the shhh.fasta and shhh.names. You will now feed these into <a href="/wiki/Trim.seqs" title="Trim.seqs">trim.seqs</a> to actually remove the barcode and primer sequences, make sure everything is 200 bp long, remove sequences with homoopolymers longer than 8 bp, and get the reverse complement for each sequence. This will also create a new <a href="/wiki/Names_file" title="Names file" class="mw-redirect">names file</a>, which maps the names of redundant sequences to a unique sequence and it will create a <a href="/wiki/Group_file" title="Group file">group file</a> that indicates what group or sample each sequence came from:
</p>
<pre>mothur &gt; trim.seqs(fasta=GQY1XT001.shhh.fasta, name=GQY1XT001.shhh.names, oligos=GQY1XT001.oligos, pdiffs=2, bdiffs=1, maxhomop=8, minlength=200, flip=T, processors=2)
</pre>
<p>Lets see what the new fasta data look like:
</p>
<pre>mothur &gt; summary.seqs(fasta=GQY1XT001.shhh.trim.fasta, name=GQY1XT001.shhh.trim.names)
</pre>
<p>To demonstrate the current option, notice that you can also run this command as follows and get the same output:
</p>
<pre>mothur &gt; summary.seqs()

		Start	End	NBases	Ambigs	Polymer	NumSeqs
Minimum:	1	247	247	0	3	1
2.5%-tile:	1	255	255	0	4	1725
25%-tile:	1	261	261	0	4	17244
Median: 	1	266	266	0	4	34488
75%-tile:	1	274	274	0	5	51732 
97.5%-tile:	1	292	292	0	6	67251
Maximum:	1	312	312	0	8	68975
Mean:	1	268.629	268.629	0	4.36483
# of unique seqs:	21886
total # of seqs:	68975
</pre>
<p>If you are ever curious what is "current" and what isn't, you can run the <a href="/wiki/Get.current" title="Get.current">get.current</a> command:
</p>
<pre>mothur &gt; get.current()
</pre>
<p><br />
</p>
<h3><span class="mw-headline" id="Using_quality_scores">Using quality scores</span></h3>
<p>If for some reason you are unable to run shhh.flows with your data, a good alternative is to use the <a href="/wiki/Trim.seqs" title="Trim.seqs">trim.seqs</a> command using a 50-bp sliding window and to trim the sequence when the average quality score over that window drops below 35.  Our results suggest that the sequencing error rates by this method are very good, but not quite as good as by shhh.flows and that the resulting sequences tend to be a bit shorter.  
</p>
<pre>mothur &gt; trim.seqs(fasta=GQY1XT001.fasta, oligos=GQY1XT001.oligos, qfile=GQY1XT001.qual, maxambig=0, maxhomop=8, flip=T, bdiffs=1, pdiffs=2, qwindowaverage=35, qwindowsize=50, processors=2)
</pre>
<p>If you are in a bind and do not have access to the quality score data the following will get you through...
</p>
<pre>mothur &gt; trim.seqs(fasta=GQY1XT001.fasta, oligos=GQY1XT001.oligos, maxambig=0, maxhomop=8, flip=T, bdiffs=1, pdiffs=2, keepfirst=200, processors=2)

</pre>
<p>Basically what we've found is that the sequence quality really nose dives after 250 bp for Titanium data.  Please send all complaints to 454, not me.  Let's take a look at what the sequences look like from this approach:
</p>
<pre>mothur &gt; summary.seqs(fasta=GQY1XT001.trim.fasta)
</pre>
<p>Again, you will have fewer sequences and they will be shorter than those obtained by the shhh.flows approach.
</p>
<h2><span class="mw-headline" id="Processing_improved_sequences">Processing improved sequences</span></h2>
<p>Again, this SOP uses the output from the shhh.flows approach.  If you used the trim.seqs approach, you will need to adjust the file names appropriately.  The next step is to simplify the dataset by working with only the unique sequences.  We aren't chucking anything here, we're just making the life of your CPU and RAM a bit easier.  We will do this with the <a href="/wiki/Unique.seqs" title="Unique.seqs">unique.seqs</a> command.  Because we already have a names file going, we will need to provide that as input with our fasta sequence:
</p>
<pre>mothur &gt; unique.seqs(fasta=GQY1XT001.shhh.trim.fasta, name=GQY1XT001.shhh.trim.names)
</pre>
<p>Looking at the fasta data we see...
</p>
<pre>mothur &gt; summary.seqs(fasta=GQY1XT001.shhh.trim.unique.fasta, name=GQY1XT001.shhh.trim.unique.names)

		Start	End	NBases	Ambigs	Polymer	NumSeqs
Minimum:	1	247	247	0	3	1
2.5%-tile:	1	255	255	0	4	1725
25%-tile:	1	261	261	0	4	17244
Median: 	1	266	266	0	4	34488
75%-tile:	1	274	274	0	5	51732
97.5%-tile:	1	292	292	0	6	67251
Maximum:	1	312	312	0	8	68975
Mean:	1	268.629	268.629	0	4.36483
# of unique seqs:	14255
total # of seqs:	68975
</pre>
<p>We have basically simplified the dataset by about 5-fold.  This means that a number of the downstream analyses will be 5 to 25 times faster than if we were working with the unique data.  Next, we need to generate an alignment of our data using the <a href="/wiki/Align.seqs" title="Align.seqs">align.seqs</a> command by aligning our data to the SILVA-compatible <a href="/wiki/Alignment_database" title="Alignment database">alignment database</a> reference alignment.  I prefer the <a href="/wiki/Silva_reference_alignment" title="Silva reference alignment" class="mw-redirect">silva reference alignment</a>, for the reasons I articulated in a recent <a rel="nofollow" class="external text" href="http://www.ncbi.nlm.nih.gov/pubmed/20011594">PLoS Computational Biology</a> paper that I published.  For now, if your computer has less than 2 GB of RAM you should probably stick with the  <a href="/w/images/7/72/Greengenes.alignment.zip" class="internal" title="Greengenes.alignment.zip"> greengenes reference alignment</a> and tell your PI to <a rel="nofollow" class="external text" href="http://www.google.com/search?client=safari&amp;rls=en&amp;q=ram&amp;ie=UTF-8&amp;oe=UTF-8#q=ram&amp;hl=en&amp;client=safari&amp;rls=en&amp;prmd=ivnsr&amp;source=univ&amp;tbs=shop:1&amp;tbo=u&amp;sa=X&amp;ei=ywtoTbuxOJC6tgftipHmAw&amp;ved=0CHwQrQQ&amp;biw=1290&amp;bih=1468&amp;bav=on.1,or.&amp;fp=61ddd4f1c4c47812">order you some more RAM</a>.
</p>
<pre>mothur &gt; align.seqs(fasta=GQY1XT001.shhh.trim.unique.fasta, reference=silva.bacteria.fasta, processors=2)
</pre>
<p>Alternatively, you can run it as...
</p>
<pre>mothur &gt; align.seqs(reference=silva.bacteria.fasta, processors=2)
</pre>
<p><br />
I takes about 150 seconds to run on my MacBook Pro.  Taking a look at the newly aligned sequences we see:
</p>
<pre>mothur &gt; summary.seqs(fasta=GQY1XT001.shhh.trim.unique.align, name=GQY1XT001.shhh.trim.unique.names)

		Start	End	NBases	Ambigs	Polymer	NumSeqs
Minimum:	15695	27656	247	0	3	1
2.5%-tile:	16116	27659	255	0	4	1725
25%-tile:	16312	27659	261	0	4	17244
Median: 	16352	27659	266	0	4	34488
75%-tile:	16421	27659	274	0	5	51732
97.5%-tile:	21280	27659	292	0	6	67251
Maximum:	21304	28432	312	0	8	68975
Mean:	17044.5	27660.5	268.629	0	4.36483
# of unique seqs:	14255
total # of seqs:	68975
</pre>
<p>We can see that pretty much all of our sequences end at position 27659 of the alignment space (the full alignment is 50,000 columns long).  We also see that 97.5% of our sequences are at least 255 bp long.
</p><p><br />
To make sure that all of the sequences overlap in the same alignment space we need to remove those sequences that are outside the desired range using the <a href="/wiki/Screen.seqs" title="Screen.seqs">screen.seqs</a> command.  There are several ways to run this command that are perfectly acceptable.  The goal is to think about and then optimize the criteria so that you have as many long sequences as possible - these two factors are inversely related to each other.  Our preferred approach is to set the start and end positions.  These parameters indicate the position by which all sequences must start by and end after.  Setting these will allow you to dictate the alignment coordinates of your sequences so that they all overlap.  We prefer this to setting a minimum length because sequences (especially in the V1 region) vary in length when they cover the same region.  We can do this as follows:
</p>
<pre>mothur &gt; screen.seqs(fasta=GQY1XT001.shhh.trim.unique.align, name=GQY1XT001.shhh.trim.unique.names, group=GQY1XT001.shhh.groups, end=27659, optimize=start, criteria=95, processors=2)
</pre>
<p><br />
What this is doing is to require all sequences to end after position 27659 and it will select a start position such that 95% of your sequences start before that position.  You can also "hard code" the start value or alter the criteria value to have more short sequences or fewer long sequences.  Play with this a bit and see what options you think work best for your dataset.  
</p>
<pre>mothur &gt; summary.seqs(fasta=GQY1XT001.shhh.trim.unique.good.align, name=GQY1XT001.shhh.trim.unique.good.names)

		Start	End	NBases	Ambigs	Polymer	NumSeqs
Minimum:	15695	27659	249	0	3	1
2.5%-tile:	16116	27659	255	0	4	1694
25%-tile:	16312	27659	262	0	4	16937
Median: 	16352	27659	267	0	4	33874
75%-tile:	16421	27659	274	0	5	50810
97.5%-tile:	21280	27659	292	0	6	66053
Maximum:	21280	28432	312	0	8	67746
Mean:	16968.9	27660.6	268.91	0	4.37131
# of unique seqs:	14107
total # of seqs:	67746
</pre>
<p>We see that all of our sequences start by position 21280 and end after position 27659.  Also, all of the sequences are at least 249 bp in length.  The sequence that is 312 bp is suspicious, but oh well, let's move on.  Next, we need to filter our alignment so that all of our sequences only overlap in the same region and to remove any columns in the alignment that don't contain data.  We do this by running the <a href="/wiki/Filter.seqs" title="Filter.seqs">filter.seqs</a> command.  Sometimes people run this and find that they don't have anything left; this is typically because of problems in the screen.seqs step above.
</p>
<pre>mothur &gt; filter.seqs(fasta=GQY1XT001.shhh.trim.unique.good.align, vertical=T, trump=., processors=2)

</pre>
<p>In this command trump=. will remove any column that has a "." character, which indicates missing data.  The vertical=T option will remove any column that contains exclusively gaps.  We should see that our alignment is now 449 columns long.  That's another pretty significant reduction in data that we have to process, plus it makes our analysis more robust.  Because we forced everything to the same alignment space with no overhangs, we probalby have a number of sequences that are now redundant over this region.  Let's run unique.seqs again to further simplify the dataset:
</p>
<pre>mothur &gt; unique.seqs(fasta=GQY1XT001.shhh.trim.unique.good.filter.fasta, name=GQY1XT001.shhh.trim.unique.good.names)

</pre>
<p>This will reduce the number of sequences from 14107 to 10467.  The final step we can take to reduce our sequencing error is to use the <a href="/wiki/Pre.cluster" title="Pre.cluster">pre.cluster</a> command to merge sequence counts that are within 2 bp of a more abundant sequence.  As a rule of thumb we use a difference of 1 bp per 100 bp of sequence length:
</p>
<pre>mothur &gt; pre.cluster(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.fasta, name=GQY1XT001.shhh.trim.unique.good.filter.names, group=GQY1XT001.shhh.good.groups, diffs=2)
</pre>
<p>This implementation of the command will split the sequences by group and then within each group it will pre-cluster those sequences that are with 1 or 2 bases of a more abundant sequence.  It will then merge all the sequences back into one file that is outputted as GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.fasta and a names file that is GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.names.  Let's see what we have left
</p>
<pre>mothur &gt; summary.seqs(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.fasta, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.names)
 
		Start	End	NBases	Ambigs	Polymer	NumSeqs
Minimum:	1	449	244	0	3	1
2.5%-tile:	1	449	251	0	4	1694
25%-tile:	1	449	254	0	4	16937
Median: 	1	449	255	0	4	33874
75%-tile:	1	449	255	0	4	50810
97.5%-tile:	1	449	255	0	6	66053
Maximum:	2	449	261	0	8	67746
Mean:	1.00006	449	254.103	0	4.3462
# of unique seqs:	6831
total # of seqs:	67746
</pre>
<p>We still have the same total number of sequences, but we now have 6,831 unique sequences.
</p><p><br />
</p>
<h2><span class="mw-headline" id="Removing_chimeras">Removing chimeras</span></h2>
<p>To this point we have been concerned with removing sequencing errors.  The next thing we need to do is to identify chimeras.  mothur has a number of methods for <a href="/wiki/Chimera.seqs" title="Chimera.seqs"> tools for detecting chimeras</a>.  Our preferred method is to use <a href="/wiki/Chimera.uchime" title="Chimera.uchime">chimera.uchime</a> using the sequences as their own reference:
</p>
<pre>mothur &gt; chimera.uchime(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.fasta, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.names, group=GQY1XT001.shhh.good.groups, processors=2)
 
...

It took 38 secs to check 8944 sequences. 4414 chimeras were found.
</pre>
<p>The above method will first split the sequences by group and then check each sequence within a group using the more abundant sequences as reference sequences.  This enables us to parallelize the entire process by putting each group onto a separate processor.  The upside of this approach is that you get to use your more abundant sequences as the reference database.  The idea is that chimeras should be rarer than their more abundant parent sequences.  By default, if <a href="/wiki/Chimera.uchime" title="Chimera.uchime">chimera.uchime</a> calls a sequence as chimeric in one group, it considers it a chimera in all samples and will flag all for removal.  
</p>
<pre>mothur &gt; remove.seqs(accnos=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.denovo.uchime.accnos, fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.fasta, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.names, group=GQY1XT001.shhh.good.groups, dups=T)

Removed 10631 sequences from your name file.
Removed 4414 sequences from your fasta file.
Removed 10631 sequences from your group file.
</pre>
<p><br />
These lines tell us that although there were 4414 unique sequences flagged as chimeras, they really represented close to 10,631 sequences that were all removed from further analysis.  Let's see what we've got left...
</p>
<pre>mothur &gt; summary.seqs(name=current)

		Start	End	NBases	Ambigs	Polymer	NumSeqs
Minimum:	1	449	245	0	3	1
2.5%-tile:	1	449	251	0	4	1428
25%-tile:	1	449	253	0	4	14279
Median: 	1	449	255	0	4	28558
75%-tile:	1	449	255	0	4	42837
97.5%-tile:	1	449	255	0	6	55688
Maximum:	2	449	261	0	8	57115
Mean:	1.00002	449	254.069	0	4.33676
# of unique seqs:	2417
total # of seqs:	57115
</pre>
<h2><span class="mw-headline" id="Removing_.22contaminants.22">Removing "contaminants"</span></h2>
<p>There is one more step we can take to improve our data quality.  Because chloroplasts and mitochondria are generally considered to be "former" bacteria and not an active component to a microbial community, the next thing we will do is to remove sequences affiliated with organelles from our dataset.  To do this, we first need to classify our sequences using the mothur version of the "Bayesian" classifier.  We can do this with the <a href="/wiki/Classify.seqs" title="Classify.seqs">classify.seqs</a> command using the RDP reference files you downloaded above:
</p>
<pre>mothur &gt; classify.seqs(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.fasta, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.names, group=GQY1XT001.shhh.good.pick.groups, template=trainset9_032012.pds.fasta, taxonomy=trainset9_032012.pds.tax, cutoff=80, processors=2)

</pre>
<p>Now let's use the <a href="/wiki/Remove.lineage" title="Remove.lineage">remove.lineage</a> command to remove those sequences that classified as "Chloroplast", "Mitochondria", or "unknown" (those sequences that could not be classified at the Kingdom level) as well as archaeal and eukaryotic 16S/18S rRNAs, since our primers are not designed to amplify these populations:
</p>
<pre>mothur &gt; remove.lineage(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.fasta, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.names, group=GQY1XT001.shhh.good.pick.groups, taxonomy=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pds.wang.taxonomy, taxon=Mitochondria-Chloroplast-Archaea-Eukaryota-unknown)
mothur &gt; summary.seqs(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.fasta, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.names)
 
		Start	End	NBases	Ambigs	Polymer	NumSeqs
Minimum:	1	449	245	0	3	1
2.5%-tile:	1	449	251	0	4	1428
25%-tile:	1	449	253	0	4	14279
Median: 	1	449	255	0	4	28557
75%-tile:	1	449	255	0	4	42835
97.5%-tile:	1	449	255	0	6	55686
Maximum:	2	449	261	0	8	57113
Mean:	1.00002	449	254.069	0	4.33675
# of unique seqs:	2415
total # of seqs:	57113
</pre>
<p><br />
While not a huge reduction, we see that there was 1 Chloroplast and 1 Mitochondrial sequence in the dataset.  Later we will come back and assess how good our data actually look, but let's kick that down the road and get going with the analysis.  If you have a good reason to think that some sequences are contaminants, you can use a similar approach to remove those.
</p>
<h2><span class="mw-headline" id="Error_analysis">Error analysis</span></h2>
<p>Before we get rolling with the actual analysis of the data, let's take a gander at the quality of our data to this point.  In the folder you pulled down was a called HMP_MOCK.v35.align which is a full alignment of the sequences that were put into the mock community and sequenced.  Because our sequences have been filtered, we need to use the filter that was generated when we ran <a href="/wiki/Filter.seqs" title="Filter.seqs">filter.seqs</a> previously to filter this alignment.  We do this as follows:
</p>
<pre>mothur &gt; filter.seqs(fasta=HMP_MOCK.v35.align, hard=GQY1XT001.filter)

</pre>
<p>Next, we need to get the sequences that correspond to our mock community (the group name is MOCK.GQY1XT001):
</p>
<pre>mothur &gt; get.groups(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.fasta, group=GQY1XT001.shhh.good.pick.pick.groups, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.names, groups=MOCK.GQY1XT001)

</pre>
<p>Now we can use the <a href="/wiki/Seq.error" title="Seq.error">seq.error</a> command to get the error rate for those sequences by comparing our sequences to the references:
</p>
<pre>mothur &gt; seq.error(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.pick.fasta, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.pick.names, reference=HMP_MOCK.v35.filter.fasta, processors=2)

Overall error rate:	6.5627e-05
Errors	Sequences
0	5912
1	7
2	3
3	12
4	5
5	3
6	1
7	0
8	0
9	1
It took 1 secs to check 96 sequences.
</pre>
<p>That's right, we've reduced the sequencing error rate from ~0.8% to 0.007%.  It should be added that this is probably a conservative estimate of error since some chimeras may have slipped through in calculating the error rate (chimeras are not sequencing errors).
</p><p>Now let's see how many chimeras <a href="/wiki/Seq.error" title="Seq.error">seq.error</a> threw out as being chimeric.  You might have to open final.pick.error.chimera in a spreadsheet...
</p>
<pre>mothur &gt; system(grep -c "2$" GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.pick.error.chimera)

53
</pre>
<p>That means that of the remaining 96 unique sequences, at least 56 of them are true chimeras that we were not able to detect.  Look for future versions of mothur to report the sensitivity, specificity, and false detection rate for chimera checking.
</p><p>Next, we want to see how many OTUs and phylotypes there should be and how many we observed in the mock community data.  To do this we need to create a file either using command line tools or a spreadsheet.  What we want is a file called mock.accnos that contains the unique sequence names contained in column 2 of the file ending "error.summary".  This is important because, just because you put a sequence in a tube doesn't mean that it got sequenced.  Here's how I did this with shell commands...
</p>
<pre>system(cut -f 2 *error.summary | sort | uniq &gt; mock.accnos)
</pre>
<p>Open the mock.accnos file in a text editor and remove the bottom line - "reference" - from the file.  Save and close.  Returning to mothur, let's get these sequecnes from the filtered fasta sequence data and calculate OTUs:
</p>
<pre>mothur &gt; get.seqs(accnos=mock.accnos, fasta=HMP_MOCK.v35.filter.fasta)
mothur &gt; dist.seqs(fasta=HMP_MOCK.v35.filter.pick.fasta, output=lt)
mothur &gt; cluster(phylip=HMP_MOCK.v35.filter.pick.phylip.dist)
</pre>
<p>Looking at the output files, we see that if there were no chimeras and no sequencing errors, there should only be 18 OTUs at a 0.03 cutoff.  Now we'll see how many OTUs there were in the mock community dataset:
</p>
<pre>mothur &gt; dist.seqs(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.pick.fasta, cutoff=0.10)
mothur &gt; cluster(column=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.pick.dist, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.pick.names)
mothur &gt; summary.single(calc=sobs, label=0.03)
mothur &gt; system(cat GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.pick.an.summary)
</pre>
<p>This tells us that there were a total of 29 OTUs - 11 of these are the product of undetected chimeras and sequencing errors.  Considering the large number of remaining chimeras not detected by chimera.uchime this isn't so bad.  Regardless, it still kinda sucks.  If we sub-sample to 4405 sequences (see below), we get...
</p>
<pre>mothur &gt; summary.single(calc=sobs, label=0.03, subsample=4419)
mothur &gt; system(cat GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.pick.an.ave-std.summary)

label	method	sobs
0.03	ave	26.227000	

</pre>
<p>This indicates that when we rarefy down to 4419 sequences, we lose 3 spurious OTUs.  
</p><p><br />
So now we know how good (or bad) our data are.  If you wanted you could start back at the step after the sffinfo command and start messing around and seeing how much this output changes.  I've done this for 90 mock communities.  What you've been doing is what I came up with.  Clearly there is room for improvement in the area of chimera prevention and detection, but I'd say that we've got the actual sequencing error down pretty well.  I'll add that the only way we can do this type of analysis is because we dedicated one barcode to a mock community.  If we were sequencing on numerous plates we could see whether there was variation in the error rates or in the relative abundance of the OTUs/phylotypes.  Finally, if your PI (or sequencing center) freaks out that you "threw away" a bunch of data, feel free to reanalyze these data however you (or he/she) see fit and show them how it changes the output.
</p><p><br />
</p>
<h2><span class="mw-headline" id="Preparing_inputs_for_analysis">Preparing inputs for analysis</span></h2>
<p>Let's remove the mock community data so we're only using our "real" data.
</p>
<pre>mothur &gt; remove.groups(fasta=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.fasta, group=GQY1XT001.shhh.good.pick.pick.groups, name=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.names, taxonomy=GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pds.wang.pick.taxonomy, groups=MOCK.GQY1XT001)
</pre>
<p>We pride ourselves on generating files with the longest possible file names.  I mean who doesn't love typing out GQY1XT001.shhh.trim.unique.good.filter.unique.precluster.pick.pick.fasta final.fasta?  Let's use the mothur <a href="/wiki/Rename.file" title="Rename.file">rename.file</a> command to make copies of some files so that we don't develop carpel tunnel syndrome.
</p>
<pre>mothur &gt; rename.file(fasta=current, name=current, group=current, prefix=final, deleteold=false)
</pre>
<p>To get ready for doing the fun part of analysis, we need to remember that there are three general approaches to analyzing 16S rRNA gene sequences.  You can analyze sequences binned into OTUs or phylotypes or you can use the phylogenetic tree-based approaches.
</p>
<h3><span class="mw-headline" id="OTUs">OTUs</span></h3>
<p><b>Option 1</b>
To build OTUs within mothur there are several options.  The SOP in the Schloss lab is to first generate a distance matrix using the <a href="/wiki/Dist.seqs" title="Dist.seqs">dist.seqs</a> command.  We will use a cutoff of 0.15, which means we'll chuck any pairwise distance larger than 0.15:
</p>
<pre>mothur &gt; dist.seqs(fasta=final.fasta, cutoff=0.15, processors=2)

</pre>
<p>If the output of this command is in the &gt;100 GBs of memory range you have probably done something incorrectly above or chosen to ignore some of my advice...  Next, we want to cluster these sequences into OTUs based on the newly created distance matrix and the names file we've been keeping track of along the way using the <a href="/wiki/Cluster" title="Cluster">cluster</a> command:
</p>
<pre>mothur &gt; cluster(column=final.dist, name=final.names)

</pre>
<p>Typically we are interested in cutoffs of 0.03 for further analysis.
</p><p><br />
<b>Option 2</b>
Generally, I'll set up the cluster command and let it rip overnight and all will be well in the morning.  If I'm in a pinch (e.g. I have to give a talk in an hour and still don't have my slides done), I'll use the cluster.split command.  It is wicked fast and the only reason I don't put it in the SOP is because I think that sometimes things that are hard are better.  Regardless, if you use a taxlevel of 2-3 (Phylum or Class) the output should be about the same and you won't owe Gaia any carbon credits:
</p>
<pre>mothur &gt; cluster.split(fasta=final.fasta, taxonomy=final.taxonomy, name=final.names, taxlevel=3, processors=4)
</pre>
<p>Like I said above, the output from Option 2 should be about the same as Option 1.  The remainder of this tutorial uses Option 1.  Now that we have a list file from one of these options, we would like to create a table that indicates the number of times an OTU shows up in each sample.  This is called a <a href="/wiki/Shared_file" title="Shared file">shared file</a> and can be created using the <a href="/wiki/Make.shared" title="Make.shared">make.shared</a> file:
</p>
<pre>mothur &gt; make.shared(list=final.an.list, group=final.groups, label=0.03)

</pre>
<p>The final step to getting good OTU data is to normalize the number of sequences in each sample.  We have observed that the number of spurious OTUs is correlated with the number of sequences.  Because of this, we standardize the number of sequences across samples so that they all have the same opportunity to be "wrong".  Most of these spurious OTUs are chimeras that we could not detect.  To do this we will pick a minimum number of sequences that a sample has and randomly select this number of sequences from each sample.  This is not the ideal solution because you could get lucky/unlucky in how you pick sequences (filed under things to worry about in the middle of the night if you've run out of other things to worry about).  First we need to know how many sequences are in each step.  You can do this with the <a href="/wiki/Count.groups" title="Count.groups">count.groups</a> command:
</p>
<pre>mothur &gt; count.groups()
</pre>
<p><br />
From the output we see that the sample with the fewest sequences had 4419 sequences in it.  That seems like a reasonable size, so let's sub-sample all the samples to this number of seqeunces:
</p>
<pre>mothur &gt; sub.sample(shared=final.an.shared, size=4419)

</pre>
<p>Typically, this will remove a few groups that that didn't have enough sequences.  Get on the horn and yell at the sequencing center or read a book about the probability of getting 96 evenly represented samples with 5000 sequences each.
</p><p>The last thing we'd like to do is to get the taxonomy information for each of our OTUs.  To do this we will use the <a href="/wiki/Classify.otu" title="Classify.otu">classify.otu</a> command to give us the majority consensus taxonomy:
</p>
<pre>mothur &gt; classify.otu(list=final.an.list, name=final.names, taxonomy=final.taxonomy, label=0.03)
</pre>
<h3><span class="mw-headline" id="Phylotype">Phylotype</span></h3>
<p>With the completion of that last step, I generally have no reason to assign sequences to phylotypes.  We do the following steps to make clinicians and people that think the names mean something happy.  The <a href="/wiki/Phylotype" title="Phylotype">phylotype</a> command goes through the <a href="/w/index.php?title=Taxonomy_file&amp;action=edit&amp;redlink=1" class="new" title="Taxonomy file (page does not exist)">taxonomy file</a> and bins sequences together that have the same taxonomy.  Here we do it to the genus level:
</p>
<pre>mothur &gt; phylotype(taxonomy=final.taxonomy, name=final.names, label=1)

</pre>
<p>Like above, we want to make a shared file and standardize the number of sequences in each group:
</p>
<pre>mothur &gt; make.shared(list=final.tx.list, group=final.groups, label=1)
mothur &gt; sub.sample(shared=final.tx.shared, size=4419)
</pre>
<p>Finally, just to keep things consistent, we get the taxonomy of each phylotype:
</p>
<pre>mothur &gt; classify.otu(list=final.tx.list, name=final.names, taxonomy=final.taxonomy, label=1)
</pre>
<h3><span class="mw-headline" id="Phylogenetic_tree">Phylogenetic tree</span></h3>
<p>There are many ways to construct phylogenetic trees.  We have ported <a href="/wiki/Clearcut" title="Clearcut">clearcut</a> into mothur to generate neighbor joining trees.  By default we do not use their heuristic, so these are real neighbor joining trees which you may or may not think are "real".  First we need to subsample the sequences from each group and then construct a phylip-formatted distance matrix, which we calculate with <a href="/wiki/Dist.seqs" title="Dist.seqs">dist.seqs</a>:
</p>
<pre>mothur &gt; dist.seqs(fasta=final.fasta, output=phylip, processors=2)
</pre>
<p>Now we call on <a href="/wiki/Clearcut" title="Clearcut">clearcut</a>:
</p>
<pre>mothur &gt; clearcut(phylip=final.phylip.dist)
</pre>
<h2><span class="mw-headline" id="Analysis">Analysis</span></h2>
<p>Moving on, let's do something more interesting and actually analyze our data.  Remember that our initial question had to do with the stability and change in community structure in these samples when comparing early and late samples.  Keep in mind that the group names have either a F or M (sex of animal) followed by a number (number of animal) followed by a D and a three digit number (number of days post weaning).  
</p><p><br />
</p>
<h3><span class="mw-headline" id="OTU-based_analysis">OTU-based analysis</span></h3>
<h4><span class="mw-headline" id="Alpha_diversity">Alpha diversity</span></h4>
<p>Let's start our analysis by analyzing the alpha diversity of the samples.  First we will generate collector's curve of the <a href="/wiki/Chao" title="Chao"> Chao1</a> richness estimators and the <a href="/wiki/Invsimpson" title="Invsimpson"> inverse Simpson</a> diversity index.  To do this we will use the <a href="/wiki/Collect.single" title="Collect.single">collect.single</a> command:
</p>
<pre>mothur &gt; collect.single(shared=final.an.shared, calc=chao-invsimpson, freq=100)
</pre>
<p>This command will generate file ending in *.chao and *.invsimpson for each sample, which can be plotted in your favorite graphing software package.  When you look at these plots you will see that the Chao1 curves continue to climb with sampling; however, the inverse Simpson diversity indices are relatively stable.  In otherwords, it isn't really possible to compare the richness of these communities based on the Chao1 index, but it is with the inverse Simpson index.  As a quick aside, it is important to point out that Chao1 is really a measure of the minimum richneess in a community, not the full richness of the community.  One method often used to get around this problem is to look at rarefaction curves describing the number of OTUs observed as a function of sampling effort.  We'll do this with the <a href="/wiki/Rarefaction.single" title="Rarefaction.single">rarefaction.single</a> command:
</p>
<pre>mothur &gt; rarefaction.single(shared=final.an.shared, calc=sobs, freq=100)
</pre>
<p>This will generate files ending in *.rarefaction, which again can be plotted in your favorite graphing software package.  Alas, rarefaction is not a measure of richness, but a measure of diversity.  If you consider two communities with the same richness, but different evenness then after sampling a large number of individuals their rarefaction curves will asymptote to the same value.  Since they have different evennesses the shapes of the curves will differ.  Therefore, selecting a number of individuals to cutoff the rarefaction curve isn't allowing a researcher to compare samples based on richness, but their diversity.  Finally, let's get a table containing the <a href="/wiki/Nseqs" title="Nseqs"> number of sequences</a>, the sample <a href="/wiki/Coverage" title="Coverage">coverage</a>, the number of <a href="/wiki/Sobs" title="Sobs"> observed OTUs</a>, and the <a href="/wiki/Invsimpson" title="Invsimpson">invsimpson</a> diversity estimate using the <a href="/wiki/Summary.single" title="Summary.single">summary.single</a> command.  To standardize everything, let's randomly select 4419 sequences from each sample 1000 times and calculate the average:
</p>
<pre>mothur &gt; summary.single(calc=nseqs-coverage-sobs-invsimpson, subsample=4419)
</pre>
<p>These data will be outputted to a table in a file called final.an.groups.ave-std.summary.  Interestingly, the sample coverages were all above 97%, indicating that we did a pretty good job of sampling the communities.  Plotting the richness or diversity of the samples would show that there was little difference between the different animals or between the early and late time points.  You could follow this up with a repeated-measures ANOVA and find that there was no significant difference based on sex or early vs. late.
</p>
<h4><span class="mw-headline" id="Beta_diversity_measurements">Beta diversity measurements</span></h4>
<p>Now we'd like to compare the membership and structure of the various samples using an OTU-based approach.  Let's start by generating a heatmap of the relative abundance of each OTU across the 24 samples using the <a href="/wiki/Heatmap.bin" title="Heatmap.bin">heatmap.bin</a> command and log2 scaling the relative abundance values.  Because there are so many OTUs, let's just look at the top 50 OTUs:
</p>
<pre>mothur &gt; heatmap.bin(shared=final.an.shared, scale=log2, numotu=50) 
</pre>
<p>This will generate an SVG-formatted file that can be visualized in Safari or manipulated in graphics software such as Adobe Illustrator.  Needless to say these heatmaps can be a bit of Rorshock.  A legend can be found at the bottom left corner of the heat map.
</p><p>Now let's calculate the similarity of the membership and structure found in the various samples and visualize those similarities in a heatmap with the <a href="/wiki/Jclass" title="Jclass"> Jaccard</a> and <a href="/wiki/Thetayc" title="Thetayc">thetayc</a> coefficients.  We will do this with the <a href="/wiki/Heatmap.sim" title="Heatmap.sim">heatmap.sim</a> command:
</p>
<pre>mothur &gt; heatmap.sim(calc=jclass-thetayc)
</pre>
<p>The output will be in two SVG-formatted files called final.an.0.03.jclass.heatmap.sim.svg and final.an.0.03.thetayc.heatmap.sim.svg.  In all of these heatmaps the red colors indicate communities that are more similar than those with black colors.
</p><p>When generating Venn diagrams we are limited by the number of samples that we can analyze simultaneously.  Let's take a look at the Venn diagrams for the first 4 time points of female 3 using the <a href="/wiki/Venn" title="Venn">venn</a> command:
</p>
<pre>mothur &gt; venn(groups=F003D000-F003D002-F003D004-F003D006)
</pre>
<p>This generates an two 4-way Venn diagrams.  This shows that there were a total of 360 OTUs observed between the 4 time points.  Only 73 of those OTUs were shared by all four time points.  We could look deeper at the shared file to see whether those OTUs were numerically rare or just had a low incidence.
</p><p>Let's use two non-parametric estimators to see what the predicted minimum number of overlapping OTUs is for the same samples using the <a href="/wiki/Summary.shared" title="Summary.shared">summary.shared</a> command:
</p>
<pre>mothur &gt; summary.shared(calc=sharedchao, groups=F003D000-F003D002-F003D004-F003D006, all=T)
</pre>
<p>Opening the final.an.0.03subsample.0.03.pick.sharedmultiple.summary file we see a prediction that over these four time points the core microbiome contained at least 84 OTUs.
</p><p>Next, let's generate a dendrogram to describe the similarity of the samples to each other.  We will generate a dendrogram using the <a href="/wiki/Jclass" title="Jclass">jclass</a> and <a href="/wiki/Thetayc" title="Thetayc">thetayc</a> calculators within the <a href="/wiki/Tree.shared" title="Tree.shared">tree.shared</a> command:
</p>
<pre>mothur &gt; tree.shared(calc=thetayc-jclass, subsample=4419)
</pre>
<p>This command generates two newick-formatted tree files - final.an.thetayc.0.03.ave.tre and final.an.jclass.0.03.ave.tre - that are the result of subsampling 4419 sequences 1000 times.  The trees can be visualized in software like TreeView or FigTree.  Inspection of the both trees shows that individuals' communities cluster with themselves to the exclusion of the others.  We can test to deterine whether the clustering within the tree is statistically significant or not using by choosing from the <a href="/wiki/Parsimony" title="Parsimony">parsimony</a>, <a href="/wiki/Unifrac.unweighted" title="Unifrac.unweighted">unifrac.unweighted</a>, or <a href="/wiki/Unifrac.weighted" title="Unifrac.weighted">unifrac.weighted</a> commands.  To run these we will first need to create a design file that indicates which treatment each sample belongs to.  There is a file called mouse.sex_time.design in the folder you downloaded that looks vaguely like this:
</p>
<pre>F003D000	F003Early
F003D002	F003Early
F003D004	F003Early
F003D006	F003Early
F003D008	F003Early
F003D142	F003Late
F003D144	F003Late
F003D146	F003Late
F003D148	F003Late
F003D150	F003Late
</pre>
<p>Using the parsimony command let's look at the pairwise comparisons.  Specifically, let's focus on the early vs. late comparisons for each mouse:
</p>
<pre>mothur &gt; parsimony(tree=final.an.thetayc.0.03.ave.tre, group=mouse.sex_time.design, groups=all)

1	F003Early-F003Late	1	0.009
</pre>
<p>There was clearly a significant difference between the clustering of the early and late time points.  Recall that this method ignores the branch length.  Let's incorporate the branch length using the unifrac commands:
</p>
<pre>mothur &gt; unifrac.weighted(tree=final.an.thetayc.0.03.ave.tre, group=mouse.sex_time.design, random=T)
</pre>
<pre>Tree#	Groups	WScore	WSig
1	F003Early-F003Late	0.861245	&lt;0.001
</pre>
<p>Clearly when we incorporate the branch length from the dendrogram and incorporate the weighting, the early and late time series are significantly different for each mouse.  Let's do the same analysis but without correcting for the weightings:
</p>
<pre>mothur &gt; unifrac.unweighted(tree=final.an.thetayc.0.03.ave.tre, group=mouse.sex_time.design, random=T, groups=all)

Tree#	Groups	UWScore	UWSig
1	F003Early-F003Late	0.965069	0.007
</pre>
<p><br />
Here we see a similar result to what we observed for the weighted UniFrac.
</p><p><br />
Another popular way of visualizing beta-diversity information is through ordination plots.  We can calculate distances between samples using the <a href="/wiki/Dist.shared" title="Dist.shared">dist.shared</a> command:
</p>
<pre>mothur &gt; dist.shared(shared=final.an.shared, calc=thetayc-jclass, subsample=4419)
</pre>
<p>The two resulting distance matrices (i.e. final.an.thetayc.0.03.lt.ave.dist and final.an.jclass.0.03.lt.ave.dist) can then be visualized using the <a href="/wiki/Pcoa" title="Pcoa">pcoa</a> or <a href="/wiki/Nmds" title="Nmds">nmds</a> plots.  Principal Coordinates (PCoA) uses an eigenvector-based approach to represent multidimensional data in as few dimesnsions as possible.  Our data is highly dimensional (~9 dimensions).
</p>
<pre>mothur &gt; pcoa(phylip=final.an.thetayc.0.03.lt.ave.dist)
mothur &gt; pcoa(phylip=final.an.jclass.0.03.lt.ave.dist)
</pre>
<p>The output of these commands are three files ending in *dist, *pcoa, and *pcoa.loadings.  The final.an.thetayc.0.03.lt.ave.pcoa.loadings file will tell you what fraction of the total variance in the data are represented by each of the axes.  In this case the first and second axis represent about 32 and 22% of the variation (54% of the total) for the thetaYC distances.  The output to the screen indicates that the R-squared between the original distance matrix and the distance between the points in 2D PCoA space was 0.54, but that if you add a third dimension the R-squared value increases to 0.97.  All in all, not bad.
</p><p>Alternatively, non-metric multidimensional scaling (NMDS) tries to preserve the distance between samples using a user defined number of dimensions.  We can run our data through NMDS with 2 dimensions with the following commands
</p>
<pre>mothur &gt; nmds(phylip=final.an.thetayc.0.03.lt.ave.dist)
mothur &gt; nmds(phylip=final.an.jclass.0.03.lt.ave.dist)
</pre>
<p>Opening the final.an.thetayc.0.03.lt.ave.nmds.stress file we can inspect the stress and R2 values, which describe the quality of the ordination.  Each line in this file represents a different iteration and the configuration obtained in the iteration with the lowest stress is reported in the final.an.thetayc.0.03.lt.ave.nmds.axes file.  In this file we find that the lowest stress value was 0.13 with an R-squared value of 0.91; that stress level is actually pretty good.  You can test what hapens with three dimensions by the following:
</p>
<pre>mothur &gt; nmds(phylip=final.an.thetayc.0.03.lt.ave.dist, mindim=3, maxdim=3)
</pre>
<p>The stress value drops to 0.05 and the R2 value goes up to 0.98.  Not bad.  In general, you would like a stress value below 0.20 and a value below 0.10 is even better.  Thus, we can conclude that, NMDS is better than PCoA.  We can plot the three dimensions of the NMDS data by plotting the contents of final.an.subsample.pick.thetayc.0.03.lt.nmds.axes.  Again, it is clear that the early and late samples cluster separately from each other.  Ultimately, ordination is a data visualization tool.  We might ask if the spatial separation that we see between the early and late plots in the NMDS plot is statistically significant.  To do this we have two statistical tools at our disposal.  The first analysis of molecular variance (<a href="/wiki/Amova" title="Amova">amova</a>), tests whether the centers of the clouds representing a group are more separated than the variation among samples of the same treatment.  This is done using the distance matrices we created earlier and does not actually use ordination.
</p><p><br />
</p>
<pre>mothur &gt; amova(phylip=final.an.thetayc.0.03.lt.ave.dist, design=mouse.sex_time.design)
  
F003Early-F003Late	Among	Within	Total
SS	0.171226	0.307108	0.478334
df	1	8	9
MS	0.171226	0.0383885

Fs:	4.46036 
p-value: 0.009*
</pre>
<p><br />  
Here we see from the AMOVA that the "cloud" early and late time points has a significantly different centroid for this mouse.  Thus, the observed separation in early and late samples is statistically significant.
</p><p>Next, we might ask which OTUs are responsible for shifting the samples along the two axes.  We can determine this by measuring the correlation of the relative abundance of each OTU with the two axes in the NMDS dataset.  We do this with the <a href="/wiki/Corr.axes" title="Corr.axes">corr.axes</a> command:
</p>
<pre>mothur &gt; corr.axes(axes=final.an.thetayc.0.03.lt.ave.nmds.axes, shared=final.an.0.03.subsample.shared, method=spearman, numaxes=3)
</pre>
<p>This command generates the final.an.0.03subsample.0.03.pick.spearman.corr.axes file.  The data for the first five OTUs look like this...
</p>
<pre>OTU	axis1	p-value	axis2	p-value	axis3	p-value	length
Otu001	-0.503030	0.131276	0.103030	0.757252	-0.927273	0.000112	1.059948
Otu002	-0.187879	0.573002	0.151515	0.649436	-0.878788	0.000814	0.911331
Otu003	0.369697	0.267391	-0.806061	0.004862	0.139394	0.675814	0.897686
Otu004	0.890909	0.000542	-0.030303	0.927565	0.321212	0.335228	0.947531
Otu005	0.284848	0.392803	0.321212	0.335228	-0.236364	0.478268	0.490085
Otu006	0.393939	0.237278	0.224242	0.501121	0.018182	0.956501	0.453656
Otu007	0.018182	0.956501	0.684848	0.028883	-0.406061	0.223155	0.796388
Otu008	-0.127273	0.702596	0.515152	0.122236	-0.042424	0.898725	0.532334
Otu009	0.212121	0.524539	-0.369697	0.267391	-0.115152	0.729753	0.441510
</pre>
<p>What these results show is that OTUs 1 and 2 are responsible for moving points in a negative direction along axis 3, whereas OTU 3 moves it along the negative direction on axis 2.  Recalling that we classified each OTU earlier, we can open final.an.0.03.cons.taxonomy to see that these first five OTUs are mainly members of the Porphyromonadaceae.  This helps to illustrate the power of OTUs over phylotypes since each of these OTUs is behaving differently.  These data can be plotted in what's known as a biplot where lines radiating from the origin (axis1=0, axis2=0, axis3=0) to the correlation values with each axis are mapped on top of the PCoA or NMDS plots.  Later, using the metastats command, we will see another method for describing which populations are responsible for differences seen between specific treatments.
</p><p>An alternative approach to building a biplot would be to provide data indicating metadata about each sample.  For example, we may know the weight, height, blood pressure, etc. of the subjects in these  samples.  For discussion purposes the file mouse.dpw.metadata is provided and looks something like this:
</p>
<pre>group	age
F003D000	0
F003D002	2 
F003D004	4 
...
</pre>
<p><br />
</p>
<pre>mothur &gt; corr.axes(axes=final.an.thetayc.0.03.lt.ave.nmds.axes, metadata=mouse.dpw.metadata, method=spearman, numaxes=3)
</pre>
<p>Opening the file mouse.dpw.spearman.corr.axes, we see:
</p>
<pre>Feature	axis1	p-value	axis2	p-value	axis3	p-value	length
dpw	0.709091	0.021666	-0.624242	0.053718	0.357576	0.283394	1.010123
</pre>
<p>Indicating that as the dpw increases the communities shift to in the positive direction along axis 1 and negatively along the axis 2.
</p>
<h4><span class="mw-headline" id="Population-level_analysis">Population-level analysis</span></h4>
<p>In addition to the use of <a href="/wiki/Corr.axes" title="Corr.axes">corr.axes</a> we can also use <a href="/wiki/Metastats" title="Metastats">metastats</a> to determine whether there are any OTUs that are differentially represented between the samples from men and women in this study.  Run the following in mothur:
</p>
<pre>mothur &gt; metastats(shared=final.an.0.03.subsample.shared, design=mouse.sex_time.design)
</pre>
<p>Looking in the first 10 OTUs from final.an.0.03subsample.0.03.pick.0.03.F003Late-F003Early.metastats file we see the following...
</p>
<pre>OTU	mean(group1)	variance(group1)	stderr(group1)	mean(group2)	variance(group2)	stderr(group2)	p-value	q-value
Otu001	0.096447	0.000336	0.008200	0.144105	0.004666	0.030548	0.164000	0.205631
Otu002	0.106811	0.000761	0.012341	0.107807	0.001182	0.015374	0.942650	0.439613
Otu003	0.127540	0.000184	0.006061	0.070152	0.000438	0.009362	0.000514	0.035404
Otu004	0.064449	0.000337	0.008206	0.064223	0.002877	0.023986	0.951729	0.439613
Otu005	0.055171	0.000044	0.002960	0.066124	0.000209	0.006463	0.151736	0.205631
...

</pre>
<p>These data tell us that OTU 3 was significantly different between the early and late samples.
</p><p><br />
</p>
<h3><span class="mw-headline" id="Phylotype-based_analysis">Phylotype-based analysis</span></h3>
<p>Phylotype-based analysis is the same as OTU-based analysis, but at a different taxonomic scale.  We will leave you on your own to replicate the OTU-based analyses described above with the phylotype data.
</p><p><br />
</p>
<h3><span class="mw-headline" id="Phylogeny-based_analysis">Phylogeny-based analysis</span></h3>
<p>OTU and phylotype-based analyses are taxonomic approaches that depend on a binning procedure.  In contrast, phylogeny-based approaches attempt similar types of analyses using a phylogenetic tree as input instead of a shared file.  Because of this difference these methods compare the genetic diversity of different communities.
</p><p><br />
</p>
<h4><span class="mw-headline" id="Alpha_diversity_2">Alpha diversity</span></h4>
<p>When using phylogenetic methods, alpha diversity is calculated as the total of the unique branch length in the tree.  This is done using the <a href="/wiki/Phylo.diversity" title="Phylo.diversity">phylo.diversity</a> command.  Because of differences in sampling depth we will rarefy the output:
</p>
<pre>mothur &gt; phylo.diversity(tree=final.phylip.tre, name=final.names, group=final.groups, rarefy=T)
</pre>
<p>This will generate a file called final.phylip.1.phylodiv.rarefaction.  
</p><p><br />
</p>
<h4><span class="mw-headline" id="Beta_diversity">Beta diversity</span></h4>
<p>The unifrac-based metrics are used to assess the similarity between two communities membership (<a href="/wiki/Unifrac.unweighted" title="Unifrac.unweighted">unifrac.unweighted</a>) and structure (<a href="/wiki/Unifrac.weighted" title="Unifrac.weighted">unifrac.weighted</a>).  We will use these metrics and generate PCoA plots to compare our samples.  There are two beta-diversity metrics that one can use - <a href="/wiki/Unifrac.unweighted" title="Unifrac.unweighted"> unweighted</a> and <a href="/wiki/Unifrac.weighted" title="Unifrac.weighted"> weighted</a>.  We will also have mothur subsample the trees 1000 times and report the average:
</p>
<pre>mothur &gt; unifrac.unweighted(tree=final.phylip.tre, name=final.names, group=final.groups, distance=lt, processors=2, random=F, subsample=4419)
mothur &gt; unifrac.weighted(tree=final.phylip.tre, name=final.names, group=final.groups, distance=lt, processors=2, random=F, subsample=4419)
</pre>
<p><br />
These commands will distance matrices (final.phylip.tre1.unweighted.ave.dist and final.phylip.tre1.weighted.ave.dist) that can be analyzed using all of the beta diversity approaches described above for the OTU-based analyses.
</p>
<h2><span class="mw-headline" id="Conclusion">Conclusion</span></h2>
<p>There are many other ways that one could analyze these data (hey, we've gotta save something for the paper!).  I encourage you to go back and change the settings, use different calculators, come up with a hypothesis using the data and test it.  If you think of an analysis that you wish mothur would do, please let us know and we'll see about adding it to the package.  There is a certain "pipeline" aspect to this analysis; however, it is also very much an art of working with sequences.  If you want to basically do everything that was described above, you can use the <a href="/w/index.php?title=Special:Upload&amp;wpDestFile=Batch.zip" class="new" title="Batch.zip"> batch file</a> and use mothur in the <a href="/wiki/Batch_mode" title="Batch mode">batch mode</a> as follows:
</p>
<pre>$ ./mothur batch
</pre>
<p>The file "batch" would contain each line you used above on its own line in a text file
</p>
<!-- 
NewPP limit report
CPU time usage: 0.170 seconds
Real time usage: 0.179 seconds
Preprocessor visited node count: 91/1000000
Preprocessor generated node count: 96/1000000
Post‐expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 - -total
-->

<!-- Saved in parser cache with key wikidb:pcache:idhash:1516-0!*!0!!en!*!* and timestamp 20190314140401 and revision id 100660
 -->
</div>									<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://mothur.org/w/index.php?title=454_SOP&amp;oldid=100660">https://mothur.org/w/index.php?title=454_SOP&amp;oldid=100660</a>"					</div>
													<div id='catlinks' class='catlinks catlinks-allhidden'></div>												<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>

			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=454+SOP" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
															<li  id="ca-nstab-main" class="selected"><span><a href="/wiki/454_SOP"  title="View the content page [c]" accesskey="c">Page</a></span></li>
															<li  id="ca-talk"><span><a href="/wiki/Talk:454_SOP"  title="Discussion about the content page [t]" accesskey="t">Discussion</a></span></li>
													</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label"><span>Variants</span><a href="#"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
															<li id="ca-view" class="selected"><span><a href="/wiki/454_SOP" >Read</a></span></li>
															<li id="ca-viewsource"><span><a href="/w/index.php?title=454_SOP&amp;action=edit"  title="This page is protected.&#10;You can view its source [e]" accesskey="e">View source</a></span></li>
															<li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=454_SOP&amp;action=history"  title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>
													</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label"><span>More</span><a href="#"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>

						<form action="/w/index.php" id="searchform">
														<div id="simpleSearch">
															<input type="search" name="search" placeholder="Search" title="Search mothur [f]" accesskey="f" id="searchInput" /><input type="hidden" value="Special:Search" name="title" /><input type="submit" name="fulltext" value="Search" title="Search the pages for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton" /><input type="submit" name="go" value="Go" title="Go to a page with this exact name if exists" id="searchButton" class="searchButton" />								</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page"  title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id='p-navigation' aria-labelledby='p-navigation-label'>
			<h3 id='p-navigation-label'>Navigation</h3>

			<div class="body">
									<ul>
													<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
													<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
													<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random page [x]" accesskey="x">Random page</a></li>
													<li id="n-help"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Help:Contents" title="The place to find out">Help</a></li>
											</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-tb' aria-labelledby='p-tb-label'>
			<h3 id='p-tb-label'>Tools</h3>

			<div class="body">
									<ul>
													<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/454_SOP" title="A list of all wiki pages that link here [j]" accesskey="j">What links here</a></li>
													<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/454_SOP" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
													<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li>
													<li id="t-print"><a href="/w/index.php?title=454_SOP&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>
													<li id="t-permalink"><a href="/w/index.php?title=454_SOP&amp;oldid=100660" title="Permanent link to this revision of the page">Permanent link</a></li>
													<li id="t-info"><a href="/w/index.php?title=454_SOP&amp;action=info" title="More information about this page">Page information</a></li>
											</ul>
							</div>
		</div>
				</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 17 August 2018, at 20:26.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="/wiki/Mothur:Privacy_policy" title="Mothur:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="/wiki/Mothur:About" title="Mothur:About">About mothur</a></li>
											<li id="footer-places-disclaimer"><a href="/wiki/Mothur:General_disclaimer" title="Mothur:General disclaimer">Disclaimers</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
											<li id="footer-poweredbyico">
															<a href="//www.mediawiki.org/"><img src="/w/resources/assets/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/w/resources/assets/poweredby_mediawiki_132x47.png 1.5x, /w/resources/assets/poweredby_mediawiki_176x62.png 2x" width="88" height="31" /></a>
													</li>
									</ul>
						<div style="clear:both"></div>
		</div>
		<script>if(window.jQuery)jQuery.ready();</script><script>if(window.mw){
mw.loader.state({"site":"loading","user":"ready","user.groups":"ready"});
}</script>
<script>if(window.mw){
mw.loader.load(["mediawiki.toc","mediawiki.action.view.postEdit","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest"],null,true);
}</script>
<script>if(window.mw){
document.write("\u003Cscript src=\"https://mothur.org/w/load.php?debug=false\u0026amp;lang=en\u0026amp;modules=site\u0026amp;only=scripts\u0026amp;skin=vector\u0026amp;*\"\u003E\u003C/script\u003E");
}</script>
<script>if(window.mw){
mw.config.set({"wgBackendResponseTime":256});
}</script>
	</body>
</html>
